{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:12:05.707315Z",
     "start_time": "2020-06-03T16:12:05.689405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "from tqdm import tqdm_notebook\n",
    "import pathlib\n",
    "import matplotlib.animation as animation\n",
    "import imageio\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.layers as layers\n",
    "from pathlib import Path\n",
    "import time \n",
    "# \n",
    "def save_fig(fig_id,path, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(path, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "# Custom Metric to load the models without problems\n",
    "import keras.backend as K\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "def kullback_leibler_divergence_test_2(y_true, y_pred):\n",
    "\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    return K.sum(y_true * abs(K.log(y_true /  y_pred)), axis=-1)\n",
    "\n",
    "custom_metr = custom_objects={\n",
    "        \"kullback_leibler_divergence_test_2\":kullback_leibler_divergence_test_2,\n",
    "        \"dice_coef\": dice_coef}\n",
    "\n",
    "# def create_path_from_title(title):\n",
    "#     path = title+\"/\"+title+\".h5\"\n",
    "#     return path\n",
    "\n",
    "# Loadcases chosen : LoadCase_7885,LoadCase_7818, LoadCase_7831, LoadCase_7852\n",
    "def save_fig(fig_id,path, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    os.makedirs(path,exist_ok=True)\n",
    "    path = os.path.join(path, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "# Define The Path of the Test Set\n",
    "# Make a function to load the Inputs from the Files as np.array to predict from it\n",
    "def create_instance(TEST_SET_PATH):\n",
    "    instances = []\n",
    "    input_path =os.path.join(TEST_SET_PATH,\"Input\")\n",
    "    input_names = os.listdir(input_path)\n",
    "    for i in input_names:\n",
    "        instance= np.load(os.path.join(input_path,i),allow_pickle = True)\n",
    "        instances.append(instance)\n",
    "    return np.array(instances),input_names\n",
    "def load_models(IDs,TRIALS_ROOT):\n",
    "    loaded_models = []\n",
    "    for ID in IDs:\n",
    "        model_path  =os.path.join(TRIALS_ROOT,ID,str(ID+\".h5\"))\n",
    "        loaded_model = tf.keras.models.load_model(model_path,custom_metr)\n",
    "        loaded_models.append(loaded_model)\n",
    "    return loaded_models\n",
    "\n",
    "def load_model(ID, MODEL_ROOT):\n",
    "    model_path  =os.path.join(MODEL_ROOT,ID,str(ID+\".h5\"))\n",
    "    loaded_model = tf.keras.models.load_model(model_path,custom_metr)\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T10:40:00.383466Z",
     "start_time": "2020-05-30T10:40:00.377445Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Specify the Paths\n",
    "# # The Data Root Dir, it contains possibly multiple folders with Loadcases\n",
    "# PROJECT_ROOT_DIR = \"F:/Thesis/inference_folder/data/\"\n",
    "# Loadcase_Folder_id = 'inference_test_solo'\n",
    "# DATA_PATH = os.path.join(PROJECT_ROOT_DIR, Loadcase_Folder_id)\n",
    "# MODIFIED_DATA_PATH = \"\"\n",
    "# # The Path to safe the Modified Data to \n",
    "# MODEL_ROOT = \"F:/Thesis/2020/TRIALS 2020/\"\n",
    "# MODEL_IDs = [\"ID_302\"]\n",
    "# SAVE_ROOT = \"F:/Thesis/2020/Kovertierer/ID_102/\"\n",
    "# os.makedirs(SAVE_ROOT,exist_ok=True)\n",
    "# TEST_SET_PATH = \"D:/Python Projects/Bachelor Thesis/fast_data_/test_set/191120_LoadCases_7813_7999/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:12:13.610181Z",
     "start_time": "2020-06-03T16:12:13.586250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the files as a list of the individual lines of the files\n",
    "def line_by_line(path_):\n",
    "  as_list=[]\n",
    "  f = open(path_, 'r')\n",
    "  for line in f:      \n",
    "      as_list.append(line.strip())\n",
    "  return as_list\n",
    "\n",
    "\n",
    "# OUT DATA\n",
    "# Get the number of nodes in the Structure\n",
    "def get_num_nodes(list_):\n",
    "  numnodes_i= [i for i, s in enumerate(list_) if 'Total # of Grids (Structural)' in s]\n",
    "  step1=list_[numnodes_i[0]].split()\n",
    "  num_nodes=(int(step1[-1]))\n",
    "  return num_nodes\n",
    "\n",
    "# Get the number of elements in the structure\n",
    "def get_num_elements(list_):\n",
    "  numelements_i= [i for i, s in enumerate(list_) if 'Total # of Elements' in s]\n",
    "  step1=list_[numelements_i[0]].split()\n",
    "  num_elements=(int(step1[-1]))\n",
    "  return num_elements\n",
    "\n",
    "\n",
    "# Input Deck Data\n",
    "# DON'T FORGET!: We start indexing at 0, so Node 132 for example is node_pos[131]\n",
    "\n",
    "# Extracting the volume fraction\n",
    "def get_vol_frac(list_):\n",
    "  volfrac_i= [i for i, s in enumerate(list_) if 'DCONSTR  ' in s]\n",
    "  volfrac_i= int(volfrac_i[0])\n",
    "  step1=list_[volfrac_i].split()\n",
    "  volfrac=(float(step1[-1]))\n",
    "  return volfrac\n",
    "\n",
    "# We get the position of nodes, in this case the element size is 2.5\n",
    "def get_nodes_pos(list_,num_nodes):\n",
    "# Defining the area, in which the nodes positions are saved (grid_area)\n",
    "  grid_start_i= [i for i, s in enumerate(list_) if '  GRID Data' in s]\n",
    "  pos= []\n",
    "  grid_start_i=int(grid_start_i[0])+2\n",
    "  grid_end_i = int(grid_start_i+int(num_nodes))\n",
    "  grid_area= list_[grid_start_i:grid_end_i]\n",
    "# Now we are accessing each line in the grid area and getting the nodes positions, \n",
    "# those have the string length of 8 characters, thats why we are accessing them through indexes\n",
    "# Also there are some cases in which the position is not 0, but some very small number like:\n",
    "# 3.553-15120, this is why we check also for \"-\" in the position\n",
    "  for i,s in enumerate(grid_area):\n",
    "  \n",
    "    x_pos= s[24:32]\n",
    "    y_pos= s[32:40]\n",
    "    if \"-\" in x_pos:\n",
    "      x_pos= 0\n",
    "    else:\n",
    "      x_pos= int(float(x_pos)/2.5)\n",
    "    if \"-\" in y_pos:\n",
    "      y_pos= 0\n",
    "    else:\n",
    "      y_pos= int(float(y_pos)/2.5)   \n",
    "    pos.append([x_pos,y_pos])  \n",
    "  return pos\n",
    "\n",
    "def get_elements_pos(list_,num_elements,nodes_pos):\n",
    "# Our Elements pos is equal to the node position of the lower left node\n",
    "# Since we read in the Nodes mentioned in the Input deck, we have to substract 1 from\n",
    "# the Node defined in the input deck, since we start indexing at 0\n",
    "# Define the area containing the elements\n",
    "  ele_start_i= [i for i, s in enumerate(list_) if '  CQUAD4 Elements' in s]\n",
    "  pos= []\n",
    "  ele_start_i=int(ele_start_i[0])+2\n",
    "  ele_end_i = int(ele_start_i+int(num_elements))\n",
    "  ele_area= list_[ele_start_i:ele_end_i]\n",
    "  for i,s in enumerate(ele_area):\n",
    "# Split the Nodes, which define the Element\n",
    "    s= s.split()\n",
    "# We only need the lower left node, which is the second node defined in the QUAD\n",
    "# We substract -1 to get the right node element in our vector, since indexing\n",
    "# This approach didnt work out, and is therefore commented out\n",
    "    # node_2 = int(s[4])-1\n",
    "    # node_2_pos=nodes_pos[node_2]\n",
    "    # x_pos=node_2_pos[0]\n",
    "    # y_pos=node_2_pos[1]\n",
    "    in_1=int(s[3])-1\n",
    "    in_2=int(s[4])-1\n",
    "    in_3=int(s[5])-1\n",
    "    in_4=int(s[6])-1\n",
    "# The Approach with the second node doesnt work for some reason\n",
    "# Thats why we just take the minimal entry for the lower left node\n",
    "    min_pos= (min(nodes_pos[in_1],nodes_pos[in_2],nodes_pos[in_3],nodes_pos[in_4]))\n",
    "    x_pos=min_pos[0]\n",
    "    y_pos=min_pos[1]\n",
    "    pos.append([x_pos,y_pos])\n",
    "    \n",
    "  return pos\n",
    "\n",
    "# Takes the nodes positions and volfrac and makes a volfrac and node matrix\n",
    "# The Volfrac matrix is just a matrix filled with the volume fraction\n",
    "# The Node matrix is a matrix containing the sorted nodes positions\n",
    "def get_nodes_volfrac_matrix(nodes_pos,volfrac):\n",
    "# Shape doesn't care about indexing, since the 0th entry counts as well\n",
    "  max_x =max(i[0] for i in nodes_pos)+1\n",
    "  max_y =max(i[1] for i in nodes_pos)+1\n",
    "  node_matrix= np.zeros((max_x,max_y))\n",
    "  for i,s in enumerate(nodes_pos):\n",
    "    i+=1\n",
    "    x_=s[0]\n",
    "    y_=s[1]\n",
    "    node_matrix[x_,y_]=i\n",
    "\n",
    "  volfrac_matrix=np.full((max_x-1,max_y-1),volfrac)\n",
    "  return node_matrix.astype(int), volfrac_matrix.astype(float),max_x,max_y\n",
    "\n",
    "# Same with the element matrix\n",
    "def get_elements_matrix(elements_pos):\n",
    "  max_x =max(i[0] for i in elements_pos)+1\n",
    "  max_y =max(i[1] for i in elements_pos)+1 \n",
    "  elements_matrix= np.zeros((max_x,max_y))\n",
    "  for i,s in enumerate(elements_pos):  \n",
    "    i+=1\n",
    "    x_=s[0]\n",
    "    y_=s[1]\n",
    "    elements_matrix[x_,y_]=i\n",
    "  return elements_matrix.astype(int),max_x,max_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:12:18.020286Z",
     "start_time": "2020-06-03T16:12:18.012346Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_el_matrix_from_path(folder_name,DATA_PATH):\n",
    "  final=[]\n",
    "  DATA_PATH = os.path.join(DATA_PATH, folder_name)\n",
    "  paths=[]\n",
    "  path_names= ['inputDeck.fem','inputDeck.disp','inputDeck.dens', 'inputDeck.out','inputDeck.strs','change.fem']\n",
    "  for path_name in path_names:\n",
    "    path_ = os.path.join(DATA_PATH, path_name)\n",
    "    paths.append(path_)\n",
    "  data_0 = []  \n",
    "  for path_ in paths:\n",
    "    data_read= line_by_line(path_)\n",
    "    data_0.append(data_read)\n",
    "# Out\n",
    "  num_nodes = get_num_nodes(data_0[3])\n",
    "  num_elements = get_num_elements(data_0[3])\n",
    "# fem\n",
    "  volfrac = get_vol_frac(data_0[5])\n",
    "  nodes_pos = get_nodes_pos(data_0[0],num_nodes)\n",
    "  elements_pos = get_elements_pos(data_0[0],num_elements,nodes_pos)\n",
    "  elements_matrix, x_shape_elements,y_shape_elements = get_elements_matrix(elements_pos)\n",
    "  nodes_matrix,volfrac_matrix,x_shape_nodes,y_shape_nodes = get_nodes_volfrac_matrix(nodes_pos,volfrac)\n",
    "  return (elements_matrix)\n",
    "    \n",
    "# element_matrix = get_el_matrix_from_path(\"LoadCase_7818\",DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T10:40:18.850395Z",
     "start_time": "2020-05-30T10:40:18.840389Z"
    }
   },
   "outputs": [],
   "source": [
    "# def create_prediction_txt(loaded_models,TEST_SET_PATH,IDs,SAVE_ROOT,element_matrix):\n",
    "# # Create the Test set\n",
    "#     test_instances,test_names = create_instance(TEST_SET_PATH)\n",
    "#     for i,s in enumerate(loaded_models):\n",
    "#         for j,t in enumerate(test_instances):\n",
    "#             DATA_FOLDER = os.path.join(SAVE_ROOT,test_names[j])\n",
    "#             os.makedirs(DATA_FOLDER,exist_ok=True)\n",
    "#             txt_title = str(str(test_names[j])+\"Model\"+str(IDs[i]))\n",
    "#             prediction = s.predict(np.array([t]))\n",
    "#             get_shape = np.shape(prediction[0][0])\n",
    "#             print(get_shape)\n",
    "#             resh = np.reshape(prediction,  (get_shape))\n",
    "#             flat_pred = np.ndarray.flatten(resh)\n",
    "#             element_matrix = np.rot90(element_matrix)\n",
    "#             flat_ele = np.ndarray.flatten(element_matrix)\n",
    "# #             merged_ = np.array([dict(x,y) for x,y in zip(resh,element_matrix)])\n",
    "#             merged_2 = np.array(list(zip(flat_ele,flat_pred)))\n",
    "            \n",
    "# #             np.ndarray.flatten(merged_2)\n",
    "#             path = os.path.join(DATA_FOLDER, txt_title + \".txt\")\n",
    "#             print(path)\n",
    "#             sort_ = sorted(merged_2, key=lambda x: x[0])\n",
    "#             np.savetxt(path, sort_, fmt='%5s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:46:33.431445Z",
     "start_time": "2020-06-03T16:46:33.418517Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_prediction_txt(model,DATA_PATH_RAW,SPLIT_DATA_PATH, model_id\n",
    "                      ,SAVE_ROOT,converter_path):\n",
    "    test_instances,test_names = create_instance(SPLIT_DATA_PATH)\n",
    "    \n",
    "    for i,s in enumerate(test_instances):\n",
    "        no_npy = test_names[i][:-4]\n",
    "        print(no_npy)\n",
    "        txt_DATA_FOLDER = os.path.join(SAVE_ROOT,no_npy)\n",
    "        os.makedirs(txt_DATA_FOLDER,exist_ok=True)\n",
    "        txt_dir = txt_DATA_FOLDER\n",
    "        with_p = os.path.join(txt_dir,\"WithP\")\n",
    "        without_p =  os.path.join(txt_dir,\"WithoutP\")\n",
    "        os.makedirs(with_p,exist_ok=True)\n",
    "        os.makedirs(without_p,exist_ok=True)\n",
    "        test_set_folder = os.path.join(DATA_PATH_RAW, no_npy)\n",
    "        print(test_set_folder)\n",
    "        input_deck = \"inputDeck.fem\"\n",
    "        change_ = \"change.fem\"\n",
    "        copyfile(test_set_folder+\"\\\\\"+input_deck, txt_dir + \"\\\\\" + input_deck)\n",
    "        copyfile(test_set_folder+\"\\\\\"+change_, txt_dir + \"\\\\\" + change_)\n",
    "        copyfile(converter_path+\"\\\\\"+\"Avdalim_OptiStruct.exe\", txt_dir + \"\\\\\" + \"Avdalim_OptiStruct.exe\")\n",
    "        element_matrix = get_el_matrix_from_path(no_npy,DATA_PATH_RAW)\n",
    "        txt_title = str(str(no_npy)+\"Model\"+str(model_id) + \".txt\")\n",
    "        t = time.time()\n",
    "        prediction = model.predict(np.array([s]))\n",
    "        elapsed = time.time() - t\n",
    "        print(\"Elapsed prediction time: \", elapsed)\n",
    "        get_shape = np.shape(prediction[0][0])\n",
    "        resh = np.reshape(prediction,  (get_shape))\n",
    "        flat_pred = np.ndarray.flatten(resh)\n",
    "        element_matrix = np.rot90(element_matrix)\n",
    "        flat_ele = np.ndarray.flatten(element_matrix)\n",
    "        merged = np.array(list(zip(flat_ele,flat_pred)))\n",
    "            \n",
    "#             np.ndarray.flatten(merged_2)\n",
    "        txt_path = os.path.join(txt_dir, txt_title)\n",
    "        sort_ = sorted(merged, key=lambda x: x[0])\n",
    "        np.savetxt(txt_path, sort_, fmt='%5s')\n",
    "        as_array = np.array([[txt_title],[input_deck]])\n",
    "        config_path = os.path.join(txt_dir, \"config.txt\")\n",
    "        np.savetxt(config_path, as_array, fmt='%5s')    \n",
    "        os.chdir(txt_dir)\n",
    "        p = subprocess.Popen(\"Avdalim_OptiStruct.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:20:28.653806Z",
     "start_time": "2020-06-03T16:20:07.789882Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# import pathlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# import keras.layers as layers\n",
    "# from pathlib import Path\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from tqdm import tqdm_notebook\n",
    "# import nbimporter\n",
    "# import data_modification_inf as Modify\n",
    "# import split_data_inf as Split\n",
    "# import create_output_txt as Outtxt\n",
    "# import subprocess\n",
    "# from shutil import copyfile\n",
    "\n",
    "\n",
    "# DATA_PATH = \"F:/Thesis/inference_folder/data/\"\n",
    "# DATA_PATH_RAW = os.path.join(DATA_PATH,\"raw\")\n",
    "# TEMP_PATH = \"F:/Thesis/inference_folder/temp/\"\n",
    "# os.makedirs(TEMP_PATH,exist_ok=True)\n",
    "# MODEL_PATH = \"F:/Thesis/inference_folder/utils/\"\n",
    "# model_ID = \"ID_302\"\n",
    "# SAVE_ROOT = \"F:/Thesis/inference_folder/data/out/\"\n",
    "# os.makedirs(TEMP_PATH,exist_ok=True)\n",
    "# MODIFIED_DATA_PATH = os.path.join(TEMP_PATH,\"modified\")\n",
    "# os.makedirs(MODIFIED_DATA_PATH,exist_ok=True)\n",
    "# SPLIT_DATA_PATH = os.path.join(TEMP_PATH,\"split\")\n",
    "# os.makedirs(SPLIT_DATA_PATH,exist_ok=True)\n",
    "# CONVERTER_PATH = \"F:/Thesis/inference_folder/utils/\"\n",
    "\n",
    "# import keras.backend as K\n",
    "# def dice_coef(y_true, y_pred, smooth=1):\n",
    "#     y_true_f = K.flatten(y_true)\n",
    "#     y_pred_f = K.flatten(y_pred)\n",
    "#     intersection = K.sum(y_true_f * y_pred_f)\n",
    "#     return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "# def kullback_leibler_divergence_test_2(y_true, y_pred):\n",
    "\n",
    "#     y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "#     y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "#     return K.sum(y_true * abs(K.log(y_true /  y_pred)), axis=-1)\n",
    "\n",
    "# custom_metr = custom_objects={\n",
    "#         \"kullback_leibler_divergence_test_2\":kullback_leibler_divergence_test_2,\n",
    "#         \"dice_coef\": dice_coef}\n",
    "# def load_model(ID, MODEL_ROOT):\n",
    "#     model_path  =os.path.join(MODEL_ROOT,str(ID+\".h5\"))\n",
    "#     loaded_model = tf.keras.models.load_model(model_path,custom_metr)\n",
    "#     return loaded_model\n",
    "# tf.keras.backend.clear_session()\n",
    "# model = load_model(model_ID,MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T16:20:50.383319Z",
     "start_time": "2020-06-03T16:20:45.057001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:/Thesis/inference_folder/data/raw\n",
      "LoadCase_7813\n",
      "F:/Thesis/inference_folder/data/raw\\LoadCase_7813\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model_1/conv2d_15/Relu (defined at C:\\Users\\Avdalim\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_38679]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9ce6eaa270a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH_RAW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m create_prediction_txt(model,DATA_PATH_RAW,SPLIT_DATA_PATH_temp, model_ID\n\u001b[1;32m----> 7\u001b[1;33m                       ,SAVE_ROOT,CONVERTER_PATH)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-757b97e5eb27>\u001b[0m in \u001b[0;36mcreate_prediction_txt\u001b[1;34m(model, DATA_PATH_RAW, SPLIT_DATA_PATH, model_id, SAVE_ROOT, converter_path)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0melement_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_el_matrix_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_npy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDATA_PATH_RAW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mtxt_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_npy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mget_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m     return self._model_iteration(\n\u001b[0;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m               \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m               total_epochs=1)\n\u001b[0m\u001b[0;32m    445\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    524\u001b[0m               *args, **kwds)\n\u001b[0;32m    525\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model_1/conv2d_15/Relu (defined at C:\\Users\\Avdalim\\Anaconda3\\envs\\Thesis\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_38679]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# SPLIT_DATA_PATH_temp = \"F:/Thesis/inference_folder/temp/split/raw/\"\n",
    "\n",
    "# DATA_PATH = \"F:/Thesis/inference_folder/data/\"\n",
    "# DATA_PATH_RAW = os.path.join(DATA_PATH,\"raw\")\n",
    "# print(DATA_PATH_RAW)\n",
    "# create_prediction_txt(model,DATA_PATH_RAW,SPLIT_DATA_PATH_temp, model_ID\n",
    "#                       ,SAVE_ROOT,CONVERTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T10:43:17.415916Z",
     "start_time": "2020-05-30T10:42:45.947663Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from shutil import copyfile\n",
    "def run_converter(TEST_COMP_SET_PATH,loadcase_names,txt_folder_path,loaded_models, IDs,converter_path):\n",
    "    for i,s in enumerate(loaded_models):\n",
    "        for j,t in enumerate(loadcase_names):\n",
    "            no_npy = t[:-4]\n",
    "            txt_dir = os.path.join(txt_folder_path,t)\n",
    "            with_p = os.path.join(txt_dir,\"WithP\")\n",
    "            without_p =  os.path.join(txt_dir,\"WithoutP\")\n",
    "            os.makedirs(with_p,exist_ok=True)\n",
    "            os.makedirs(without_p,exist_ok=True)\n",
    "            test_set_folder = os.path.join(TEST_COMP_SET_PATH, no_npy)\n",
    "            input_deck = \"inputDeck.fem\"\n",
    "            change_ = \"change.fem\"\n",
    "            copyfile(test_set_folder+\"\\\\\"+input_deck, txt_dir + \"\\\\\" + input_deck)\n",
    "            copyfile(test_set_folder+\"\\\\\"+change_, txt_dir + \"\\\\\" + change_)\n",
    "            copyfile(converter_path+\"\\\\\"+\"Avdalim_OptiStruct.exe\", txt_dir + \"\\\\\" + \"Avdalim_OptiStruct.exe\")\n",
    "            converter = str(os.path.join(txt_dir,\"Avdalim_OptiStruct.exe\"))\n",
    "#             print(converter)\n",
    "#             subprocess.call([converter])\n",
    "            os.chdir(txt_dir)\n",
    "            p = subprocess.Popen(\"Avdalim_OptiStruct.exe\")\n",
    "            \n",
    "            txt_title = str(str(test_names[j])+\"Model\"+str(IDs[i])+\".txt\")\n",
    "            b = \"inputDeck.fem\"\n",
    "            as_array = np.array([[txt_title],[b]])\n",
    "            path = os.path.join(txt_dir, \"config.txt\")\n",
    "            np.savetxt(path, as_array, fmt='%5s')\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
