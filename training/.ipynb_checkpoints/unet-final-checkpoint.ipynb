{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import regularizers\n",
    "import json\n",
    "\n",
    "# Insert Dice Coefficient as another metric\n",
    "\n",
    "import keras.backend as K\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Used later for Saving different Plots\n",
    "def save_fig(fig_id,path, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(path, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Used to make a Test Set Patch to get Visual Predictions\n",
    "def create_instances(test_folder):\n",
    "    instances = []\n",
    "    input_path =os.path.join(test_folder,\"Input\")\n",
    "    input_names = os.listdir(input_path)\n",
    "    for i in input_names:\n",
    "        loaded_array_x = np.load(os.path.join(input_path,i),allow_pickle = True)\n",
    "        instances.append(loaded_array_x)\n",
    "    return np.array(instances),input_names\n",
    "\n",
    "def get_paths(DATA_PATH,LOADCASE_FOLDERS):\n",
    "    INPUT_PATHS = []\n",
    "    OUTPUT_PATHS = []\n",
    "# Iterate through the Loadcase folders\n",
    "    for i in LOADCASE_FOLDERS:\n",
    "# Get the Input/Output root for each of those loadcase folders\n",
    "        INPUT_ROOTS = os.path.join(DATA_PATH,i,\"Input\")\n",
    "        OUTPUT_ROOTS = os.path.join(DATA_PATH,i,\"Output\")\n",
    "# Get the individual names of the inputs and outputs\n",
    "# I didn't use the same names, since there might be a need to keep them seperated,\n",
    "# if there is an Input but no output data, since there are not enough iterations and such\n",
    "        input_names = os.listdir(INPUT_ROOTS)\n",
    "        output_names = os.listdir(OUTPUT_ROOTS)\n",
    "        for i in input_names:\n",
    "            INPUT_PATHS.append(os.path.join(INPUT_ROOTS,i))\n",
    "        for i in output_names:\n",
    "            OUTPUT_PATHS.append(os.path.join(OUTPUT_ROOTS,i))\n",
    "    COMBINED_PATHS = list(zip(INPUT_PATHS,OUTPUT_PATHS))\n",
    "# Input is [0] Output is [1]\n",
    "    return COMBINED_PATHS\n",
    "\n",
    "\n",
    "\n",
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, input_paths, output_paths, output_dim=(64,48), output_channels = 1,\n",
    "                 batch_size=32, input_dim=(65, 49), input_channels=6, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "#     Get the Output Information\n",
    "        self.output_paths = output_paths\n",
    "        self.output_dim = output_dim\n",
    "        self.output_channels= output_channels\n",
    "#    Get the Input Information\n",
    "        self.input_paths = input_paths\n",
    "        self.input_dim =input_dim\n",
    "        self.input_channels = input_channels\n",
    "#   Shuffle Parameter\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "#   Number of batches per epoch\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.input_paths) / self.batch_size))\n",
    "#   Generates and Loads the Data\n",
    "#   With getitem we can access the data through indexing, instead of calling a method with number\n",
    "#   In this case we access the whole minibatch\n",
    "    def __getitem__(self, index):\n",
    "#   Generate indexes of the batch, index gets decided randomly \n",
    "#   The self.indexes are randomly shuffled\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "#   Make temporary path lists from the indexes with len = Batch Size\n",
    "        input_paths_temp = [self.input_paths[k] for k in indexes]\n",
    "        output_paths_temp = [self.output_paths[k] for k in indexes]\n",
    "\n",
    "#    Call the data generation method(in this case load)\n",
    "        X, y = self.__data_generation(input_paths_temp,output_paths_temp)\n",
    "\n",
    "        return X, y\n",
    "#   Shuffles the indexes after each epoch\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.input_paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self,input_paths_temp,output_paths_temp):\n",
    "#    Placeholders --- necessary?\n",
    "        X = np.empty((self.batch_size, self.input_channels, *self.input_dim), dtype= float)\n",
    "        y = np.empty((self.batch_size, self.output_channels,*self.output_dim), dtype=float)\n",
    "\n",
    "        for i,path in enumerate(input_paths_temp):\n",
    "            loaded_array_x = np.load(path,allow_pickle=True)\n",
    "            X[i,] = loaded_array_x\n",
    "        for i,path in enumerate(output_paths_temp):\n",
    "            loaded_array_y = np.load(path,allow_pickle=True)\n",
    "            y[i,] = loaded_array_y\n",
    "         \n",
    "\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "\n",
    "# Create the Model\n",
    "\n",
    "def create_unet(IMG_HEIGHT_INPUT,IMG_WIDTH_INPUT,IMG_CHANNELS_INPUT,\n",
    "                IMG_CHANNELS_OUTPUT,l2_1 = 0.0,l2_2=0.0):\n",
    "\n",
    "    def encoding_block(input_,depth, l2_ = 0.0):\n",
    "        BN_1 = layers.BatchNormalization(axis=1)(input_)\n",
    "        conv_1= layers.Conv2D(depth, (3, 3), activation='relu',\n",
    "                              kernel_regularizer = regularizers.l2(l2_),\n",
    "                              # bias_regularizer=regularizers.l2(l2_),\n",
    "                              data_format='channels_first',padding=\"same\")(BN_1)\n",
    "        BN_2 = layers.BatchNormalization(axis=1)(conv_1)\n",
    "        output_ = layers.MaxPool2D(pool_size=2,strides=2,data_format='channels_first')(BN_2)\n",
    "        return output_\n",
    "    def decoding_block(input_,depth,l2_ = 0.0):\n",
    "        Trans_1= layers.Conv2DTranspose(depth, (2,2),strides=(2, 2), \n",
    "                              kernel_regularizer = regularizers.l2(l2_),\n",
    "                              # bias_regularizer=regularizers.l2(l2_),\n",
    "                                data_format='channels_first')(input_)\n",
    "        BN_1 = layers.BatchNormalization(axis=1)(Trans_1)\n",
    "        conv_1= layers.Conv2D(depth, (7, 7), activation='relu',\n",
    "                              kernel_regularizer = regularizers.l2(l2_),\n",
    "                              # bias_regularizer=regularizers.l2(l2_),\n",
    "                              data_format='channels_first',padding=\"same\")(BN_1)\n",
    "        output_ = layers.BatchNormalization(axis=1)(conv_1)\n",
    "        return output_\n",
    "    def bottleneck (input_,depth,l2_ = 0.0):\n",
    "        conv_1 =layers.Conv2D(depth, (7, 7), activation='relu',\n",
    "                              kernel_regularizer = regularizers.l2(l2_),\n",
    "                              # bias_regularizer=regularizers.l2(l2_),\n",
    "                              data_format='channels_first',padding=\"same\")(input_)\n",
    "        BN_1= layers.BatchNormalization(axis=1)(conv_1)\n",
    "        conv_2 =layers.Conv2D(depth, (7, 7), activation='relu',\n",
    "                              kernel_regularizer = regularizers.l2(l2_),\n",
    "                              # bias_regularizer=regularizers.l2(l2_),\n",
    "                              data_format='channels_first',padding=\"same\")(BN_1)\n",
    "        output_ = layers.BatchNormalization(axis=1)(conv_2)\n",
    "        return output_\n",
    "\n",
    "    inputs = layers.Input((IMG_CHANNELS_INPUT, IMG_HEIGHT_INPUT, IMG_WIDTH_INPUT)) \n",
    "    conv_0a = layers.Conv2D(32, (2, 2), activation='relu',\n",
    "                              kernel_regularizer = regularizers.l2(l2_1),\n",
    "                              # bias_regularizer=regularizers.l2(l2_1),\n",
    "                            data_format='channels_first',padding=\"valid\")(inputs)\n",
    "    BN_1 = layers.BatchNormalization(axis=1)(conv_0a)\n",
    "    conv_enc1 =  layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                              kernel_regularizer = regularizers.l2(l2_1),\n",
    "                              # bias_regularizer=regularizers.l2(l2_1),\n",
    "                            data_format='channels_first',padding=\"same\")(BN_1)\n",
    "    enc_1 = encoding_block(conv_enc1,64,l2_1)\n",
    "    conv_enc2 =  layers.Conv2D(128, (3, 3), activation='relu',\n",
    "                              kernel_regularizer = regularizers.l2(l2_2),\n",
    "                              # bias_regularizer=regularizers.l2(l2_2),\n",
    "                            data_format='channels_first',padding=\"same\")(enc_1)\n",
    "    enc_2 = encoding_block(conv_enc2,128,l2_2)\n",
    "    conv_enc3 =  layers.Conv2D(256, (3, 3), activation='relu',\n",
    "                              kernel_regularizer = regularizers.l2(l2_2),\n",
    "                              # bias_regularizer=regularizers.l2(l2_2),\n",
    "                            data_format='channels_first',padding=\"same\")(enc_2)\n",
    "    enc_3 = encoding_block(conv_enc3,256,l2_2)\n",
    "    bottleneck_ = bottleneck(enc_3,256,l2_2)\n",
    "    dec_1 = decoding_block(bottleneck_,128,l2_2)\n",
    "    u_1 = layers.concatenate([dec_1,enc_2],axis=1)\n",
    "    dec_2 = decoding_block(u_1,64,l2_1)\n",
    "    u_2 = layers.concatenate([dec_2,enc_1],axis=1)\n",
    "    dec_3 = decoding_block(u_2,32,l2_1)\n",
    "    u_3 = layers.concatenate([dec_3,BN_1],axis=1)\n",
    "    conv_e1 =layers.Conv2D(32, (7, 7), activation='relu',\n",
    "                                kernel_regularizer = regularizers.l2(l2_1),\n",
    "                              # bias_regularizer=regularizers.l2(l2_1),\n",
    "                              data_format='channels_first',padding=\"same\")(u_3)\n",
    "    BN_e1 = layers.BatchNormalization(axis=1)(conv_e1)\n",
    "    conv_e2 =layers.Conv2D(32, (7, 7), activation='relu',\n",
    "                              kernel_regularizer = regularizers.l2(l2_1),\n",
    "                              # bias_regularizer=regularizers.l2(l2_1),\n",
    "                              data_format='channels_first',padding=\"same\")(BN_e1)\n",
    "    BN_e2 = layers.BatchNormalization(axis=1)(conv_e2)\n",
    "    outputs = layers.Conv2D(IMG_CHANNELS_OUTPUT, (1, 1), activation='sigmoid',\n",
    "                                     data_format='channels_first',padding=\"same\")(BN_e2)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Train multiple models in a Loop, with example IDs and values\n",
    "\n",
    "ID_LOOP = [\"ID_319\",\"ID_320\",\"ID_321\"]\n",
    "BATCH_SIZE_LOOP = [32,64,128]\n",
    "l2_1 = 1e-7\n",
    "l2_2 = 2e-7\n",
    "\n",
    "for i,s in enumerate(ID_LOOP):\n",
    "\n",
    "    random_seed = 42\n",
    "    np.random.seed = random_seed\n",
    "    TRIAL_ID = s\n",
    "    # The Paths of the Training Data\n",
    "    DATA_ROOT = \"/kaggle/input/rotated-dataset/\"\n",
    "    DATA_ID = \"split_rot\"\n",
    "    DATA_PATH  = os.path.join(DATA_ROOT,DATA_ID)\n",
    "    LOADCASE_FOLDERS = os.listdir(DATA_PATH)\n",
    "\n",
    "    # Define the Roots for the Callpacks and more (TRIAL ID dependant path)\n",
    "    SAVING_ROOT= \"/kaggle/working/TRIALS\"\n",
    "    TRIAL_PATH = os.path.join(SAVING_ROOT,TRIAL_ID)\n",
    "    os.makedirs(TRIAL_PATH,exist_ok=True)\n",
    "\n",
    "    # Tensorboard Path\n",
    "    TENSORBOARD_PATH = os.path.join(TRIAL_PATH,\"tensorboard\")\n",
    "    os.makedirs(TENSORBOARD_PATH,exist_ok=True)\n",
    "\n",
    "    # Checkpoint Path\n",
    "    CHECKPOINT_PATH = os.path.join(TRIAL_PATH,\"Model_Checkpoint\")\n",
    "    text=\"checkpoint.hdf5\"\n",
    "    cp_path = os.path.join(TRIAL_PATH,text)\n",
    "\n",
    "    # Define the Paths to save the models to\n",
    "    models_dir = os.path.join(TRIAL_PATH,\"models\")\n",
    "    os.makedirs(models_dir,exist_ok=True)\n",
    "    name = TRIAL_ID + \".h5\"\n",
    "\n",
    "    # History Saving Path\n",
    "    title = str (\"history_\"+TRIAL_ID)\n",
    "    HISTORY_PATH =os.path.join(TRIAL_PATH,title)\n",
    "\n",
    "    # Test Set Path to Visualize the Predictions \n",
    "    TEST_SET_PATH = \"/kaggle/input/rotated-test-set/test_set_rot\"\n",
    "\n",
    "    # Define the Dimensions of the In and Output\n",
    "    IMG_HEIGHT_INPUT= 49\n",
    "    IMG_WIDTH_INPUT = 65\n",
    "    IMG_CHANNELS_INPUT= 6\n",
    "    IMG_HEIGHT_OUTPUT = 48\n",
    "    IMG_WIDTH_OUTPUT = 64\n",
    "    IMG_CHANNELS_OUTPUT = 1\n",
    "\n",
    "\n",
    "    # Regularizer Parameters\n",
    "\n",
    "    l2_1 = l2_1\n",
    "    l2_2 = l2_2\n",
    "\n",
    "\n",
    "    # Training Hyperparameters\n",
    "\n",
    "    BATCH_SIZE_ = BATCH_SIZE_LOOP[i]\n",
    "    shuffle_data_ = True\n",
    "    test_set_fraction = 0.33\n",
    "    loss_ = \"binary_crossentropy\"\n",
    "    optimizer_ = keras.optimizers.Adam()\n",
    "    metrics_ = [\"binary_accuracy\",dice_coef]\n",
    "    epochs_ = 20\n",
    "    \n",
    "    parameters_generator = {'output_dim': (IMG_HEIGHT_OUTPUT,IMG_WIDTH_OUTPUT),\n",
    "                            'output_channels': IMG_CHANNELS_OUTPUT,\n",
    "                            'batch_size': BATCH_SIZE_,\n",
    "                            'input_dim': (IMG_HEIGHT_INPUT,IMG_WIDTH_INPUT),\n",
    "                            'input_channels': IMG_CHANNELS_INPUT,\n",
    "                            'shuffle': shuffle_data_\n",
    "    }\n",
    "\n",
    "    \n",
    "    PATHS = get_paths(DATA_PATH,LOADCASE_FOLDERS)\n",
    "    print(\"Number of found paths: \"len(PATHS))\n",
    "    \n",
    "    # Create the Generators\n",
    "\n",
    "    # Split the Data into Test and Valids\n",
    "    X_paths_train,X_paths_valid,Y_paths_train,Y_paths_valid = train_test_split(\n",
    "    # Have to unwrap the lists\n",
    "        [path_[0] for path_ in PATHS], [path_[1] for path_ in PATHS], test_size=test_set_fraction,\n",
    "        random_state=random_seed)\n",
    "\n",
    "    training_generator = DataGenerator(X_paths_train, Y_paths_train, **parameters_generator)\n",
    "    validation_generator = DataGenerator(X_paths_valid, Y_paths_valid, **parameters_generator)\n",
    "\n",
    "    # Callbacks\n",
    "\n",
    "    early_stop_cb= tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(TENSORBOARD_PATH)\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(cp_path, save_best_only=True,\n",
    "                                                       monitor='val_loss'\n",
    "                                                      )\n",
    "\n",
    "    callbacks_ = [checkpoint_cb,tensorboard_cb]\n",
    "    u_net = create_unet(IMG_HEIGHT_INPUT,IMG_WIDTH_INPUT,IMG_CHANNELS_INPUT,IMG_CHANNELS_OUTPUT,\n",
    "                    l2_1,l2_1)\n",
    "    u_net.compile(loss=loss_,\n",
    "                       optimizer=keras.optimizers.Adam(),\n",
    "                    metrics=metrics_)\n",
    "    history = u_net.fit_generator(generator=training_generator,epochs = epochs_ ,callbacks=callbacks_,\n",
    "                        validation_data=validation_generator,verbose=1)\n",
    "\n",
    "    u_net.save(os.path.join(models_dir,name))\n",
    "\n",
    "    \n",
    "    # Save the History of the Loss and the Metrics\n",
    "\n",
    "    # Get the dictionary containing each metric and the loss for each epoch\n",
    "    history_dict = history.history\n",
    "    # Save it under the form of a json file\n",
    "    json.dump(str(history_dict), open(HISTORY_PATH, 'w'))\n",
    "    \n",
    "    plt.plot(history.history['binary_accuracy'])\n",
    "    plt.plot(history.history['val_binary_accuracy'])\n",
    "    plt.title('Model binary_accuracy')\n",
    "    plt.ylabel('binary_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    save_fig(TRIAL_ID+\"_binary_accuracy\",TRIAL_PATH)\n",
    "    img_ = plt.show()\n",
    "\n",
    "    plt.plot(history.history['dice_coef'])\n",
    "    plt.plot(history.history['val_dice_coef'])\n",
    "    plt.title('Model dice coefficent')\n",
    "    plt.ylabel('dice coefficent')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    save_fig(TRIAL_ID+\"_dice coefficent\",TRIAL_PATH)\n",
    "    img_ = plt.show()\n",
    "    # Plot training & validation loss values\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    save_fig(TRIAL_ID+\"_loss\",TRIAL_PATH)\n",
    "    img_ = plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
